{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import sys"
      ],
      "metadata": {
        "id": "4CZrIxGkYihP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ii_bfchYmJy",
        "outputId": "035897e8-3c4f-42f7-9fd5-716582facfa7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset paths\n",
        "base_path = '/content/drive/MyDrive/DATASET'\n",
        "splits = {\n",
        "    'train': os.path.join(base_path, 'Train'),\n",
        "    'validate': os.path.join(base_path, 'Validate'),\n",
        "    'test': os.path.join(base_path, 'Test')\n",
        "}\n",
        "sigma_levels = [10]  # Only σ = 10"
      ],
      "metadata": {
        "id": "1BVUZX8CYnl8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify folder existence\n",
        "for split, path in splits.items():\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Folder not found: {path}\")\n",
        "    noisy_path = os.path.join(path, f'noisy_sigma{sigma_levels[0]}')\n",
        "    if not os.path.exists(noisy_path):\n",
        "        raise FileNotFoundError(f\"Noisy folder not found: {noisy_path}\")\n",
        "    print(f\"Found {split} folder: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLqfzcGGYp2X",
        "outputId": "3349eedc-495a-4b00-b5d9-8c72ff153b20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found train folder: /content/drive/MyDrive/DATASET/Train\n",
            "Found validate folder: /content/drive/MyDrive/DATASET/Validate\n",
            "Found test folder: /content/drive/MyDrive/DATASET/Test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PairedNoisyCleanDataset for RGB images\n",
        "class PairedNoisyCleanDataset(Dataset):\n",
        "    def __init__(self, base_dir, split, transform=None):\n",
        "        self.transform = transform\n",
        "        split_dir = os.path.join(base_dir, split.capitalize())\n",
        "        noisy_dir = os.path.join(split_dir, 'noisy_sigma10')\n",
        "\n",
        "        # Collect clean images (excluding noisy_sigma10)\n",
        "        self.clean_images = []\n",
        "        for root, _, files in os.walk(split_dir):\n",
        "            if os.path.abspath(root) == os.path.abspath(noisy_dir):\n",
        "                continue\n",
        "            for f in files:\n",
        "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "                    self.clean_images.append(os.path.join(root, f))\n",
        "\n",
        "        # Collect noisy images\n",
        "        self.noisy_images = [\n",
        "            os.path.join(noisy_dir, f) for f in os.listdir(noisy_dir)\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
        "        ]\n",
        "\n",
        "        # Match pairs by filename\n",
        "        clean_dict = {os.path.basename(p): p for p in self.clean_images}\n",
        "        noisy_dict = {os.path.basename(p): p for p in self.noisy_images}\n",
        "        self.matched_clean = []\n",
        "        self.matched_noisy = []\n",
        "        for fname in noisy_dict:\n",
        "            if fname in clean_dict:\n",
        "                self.matched_clean.append(clean_dict[fname])\n",
        "                self.matched_noisy.append(noisy_dict[fname])\n",
        "            else:\n",
        "                print(f\"Warning: No matching clean image for noisy file {fname}\")\n",
        "\n",
        "        print(f\"[{split}] Found {len(self.matched_clean)} paired samples\")\n",
        "        if len(self.matched_clean) == 0:\n",
        "            raise ValueError(f\"No matched pairs found in {split} dataset!\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.matched_clean)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        noisy_img = Image.open(self.matched_noisy[idx]).convert('RGB')\n",
        "        clean_img = Image.open(self.matched_clean[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            seed = torch.randint(0, 2**31, (1,)).item()\n",
        "            torch.manual_seed(seed)\n",
        "            noisy_img = self.transform(noisy_img)\n",
        "            torch.manual_seed(seed)\n",
        "            clean_img = self.transform(clean_img)\n",
        "\n",
        "        return noisy_img, clean_img\n"
      ],
      "metadata": {
        "id": "V02tl4BqYrnF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(64),  # 64x64 crops as per document\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()  # [3, 64, 64], [0, 1]\n",
        "])\n",
        "val_test_transform = transforms.ToTensor()  # [3, 256, 256]"
      ],
      "metadata": {
        "id": "jxYu1PZxYuPS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets and dataloaders\n",
        "batch_size = 16\n",
        "train_dataset = PairedNoisyCleanDataset(base_path, 'train', train_transform)\n",
        "val_dataset = PairedNoisyCleanDataset(base_path, 'validate', val_test_transform)\n",
        "test_dataset = PairedNoisyCleanDataset(base_path, 'test', val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")  # Expected: 7,999\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")  # Expected: 1,002\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")  # Expected: 1,004"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-880V57YwCG",
        "outputId": "8d24936c-01c1-4ab7-9a8e-dbe8b46595f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Found 7998 paired samples\n",
            "[validate] Found 1001 paired samples\n",
            "[test] Found 1003 paired samples\n",
            "Train dataset size: 7998\n",
            "Validation dataset size: 1001\n",
            "Test dataset size: 1003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Inspect batch structure\n",
        "for batch in train_loader:\n",
        "    print(\"Batch type:\", type(batch))\n",
        "    print(\"Batch length:\", len(batch))\n",
        "    for i, item in enumerate(batch):\n",
        "        print(f\"Item {i} shape: {item.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA4QPlnpYyPT",
        "outputId": "272564c3-c981-432a-ff95-463d1266ff0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch type: <class 'list'>\n",
            "Batch length: 2\n",
            "Item 0 shape: torch.Size([16, 3, 64, 64])\n",
            "Item 1 shape: torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bi-MSAAE Model\n",
        "class MultiScaleEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Depthwise separable convolution for 3x3\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 3, kernel_size=3, padding=1, groups=3),  # Depthwise\n",
        "            nn.Conv2d(3, 32, kernel_size=1),  # Pointwise\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Depthwise separable convolution for 5x5\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 3, kernel_size=5, padding=2, groups=3),  # Depthwise\n",
        "            nn.Conv2d(3, 32, kernel_size=1),  # Pointwise\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f1 = self.branch3x3(x)  # [batch, 32, H, W]\n",
        "        f2 = self.branch5x5(x)  # [batch, 32, H, W]\n",
        "        return torch.cat([f1, f2], dim=1)  # [batch, 64, H, W]\n",
        "\n",
        "class NoiseGateModule(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = self.conv(x)  # [batch, 1, H, W]\n",
        "        mask = self.sigmoid(mask)\n",
        "        return x * mask  # [batch, 64, H, W]\n",
        "\n",
        "class ChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.conv = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.avg_pool(x)  # [batch, 64, 1, 1]\n",
        "        max_out = self.max_pool(x)  # [batch, 64, 1, 1]\n",
        "        pooled = torch.cat([avg_out, max_out], dim=1)  # [batch, 128, 1, 1]\n",
        "        attn = self.conv(pooled)  # [batch, 64, 1, 1]\n",
        "        attn = self.sigmoid(attn)\n",
        "        return x * attn  # [batch, 64, H, W]\n",
        "\n",
        "class DualHeadDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.structural_head = nn.Conv2d(in_channels, 3, kernel_size=3, padding=1)\n",
        "        self.texture_head = nn.Conv2d(in_channels, 3, kernel_size=5, padding=2)\n",
        "        self.alpha = 0.6\n",
        "\n",
        "    def forward(self, x):\n",
        "        struct = self.structural_head(x)  # [batch, 3, H, W]\n",
        "        text = self.texture_head(x)  # [batch, 3, H, W]\n",
        "        return self.alpha * struct + (1 - self.alpha) * text  # [batch, 3, H, W]\n",
        "\n",
        "class BiMSAAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = MultiScaleEncoder()\n",
        "        self.noise_gate = NoiseGateModule()\n",
        "        self.attention = ChannelAttentionBlock()\n",
        "        self.decoder = DualHeadDecoder()\n",
        "        # Skip connection convolution to match dimensions\n",
        "        self.skip_conv = nn.Conv2d(64, 64, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        f = self.encoder(x)  # [batch, 64, H, W]\n",
        "        # Skip connection\n",
        "        skip = self.skip_conv(f)\n",
        "        # Noise-Gate\n",
        "        f = self.noise_gate(f)  # [batch, 64, H, W]\n",
        "        # Attention\n",
        "        f = self.attention(f)  # [batch, 64, H, W]\n",
        "        # Add skip connection\n",
        "        f = f + skip\n",
        "        # Decoder\n",
        "        out = self.decoder(f)  # [batch, 3, H, W]\n",
        "        return torch.sigmoid(out)  # Ensure [0, 1] output"
      ],
      "metadata": {
        "id": "jQGd3jDfY5el"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Animated Epoch Logger\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "CYAN = \"\\033[96m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def animated_epoch_update(epoch, num_epochs, train_loss, val_loss, psnr, ssim, epoch_time):\n",
        "    bar_len = 30\n",
        "    filled_len = int(round(bar_len * (epoch + 1) / num_epochs))\n",
        "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
        "    sys.stdout.write(f\"\\r{CYAN}Epoch {epoch+1}/{num_epochs} [{bar}]{RESET}\")\n",
        "    sys.stdout.flush()\n",
        "    time.sleep(0.1)\n",
        "    print(f\"\\n  🏋️  {YELLOW}Train Loss{RESET} : {train_loss:.4f}\")\n",
        "    print(f\"  📉  {YELLOW}Val Loss  {RESET} : {val_loss:.4f}\")\n",
        "    print(f\"  📷  {YELLOW}PSNR      {RESET} : {psnr:.2f} dB\")\n",
        "    print(f\"  🔍  {YELLOW}SSIM      {RESET} : {ssim:.4f}\")\n",
        "    print(f\"  ⏱️  {YELLOW}Time/Epoch{RESET}: {epoch_time:.2f} sec\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "mI9tlNaSZLYT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs):\n",
        "    model.to(device)\n",
        "    train_dataset = train_loader.dataset\n",
        "    val_dataset = val_loader.dataset\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            noisy, clean = batch\n",
        "            noisy, clean = noisy.to(device), clean.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(noisy)\n",
        "            loss = criterion(outputs, clean)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * noisy.size(0)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, psnr_val, ssim_val = 0.0, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for noisy, clean in val_loader:\n",
        "                noisy, clean = noisy.to(device), clean.to(device)\n",
        "                outputs = model(noisy)\n",
        "                loss = criterion(outputs, clean)\n",
        "                val_loss += loss.item() * noisy.size(0)\n",
        "                outputs_np = outputs.cpu().numpy()\n",
        "                clean_np = clean.cpu().numpy()\n",
        "                for i in range(outputs_np.shape[0]):\n",
        "                    psnr_channels = [\n",
        "                        peak_signal_noise_ratio(clean_np[i, c], outputs_np[i, c], data_range=1.0)\n",
        "                        for c in range(3)\n",
        "                    ]\n",
        "                    ssim_channels = [\n",
        "                        structural_similarity(clean_np[i, c], outputs_np[i, c], data_range=1.0)\n",
        "                        for c in range(3)\n",
        "                    ]\n",
        "                    psnr_val += sum(psnr_channels) / 3\n",
        "                    ssim_val += sum(ssim_channels) / 3\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataset)\n",
        "        avg_val_loss = val_loss / len(val_dataset)\n",
        "        avg_psnr_val = psnr_val / len(val_dataset)\n",
        "        avg_ssim_val = ssim_val / len(val_dataset)\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        animated_epoch_update(epoch, num_epochs, avg_train_loss, avg_val_loss, avg_psnr_val, avg_ssim_val, epoch_time)"
      ],
      "metadata": {
        "id": "zphDX9Y8ZO-T"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BiMSAAE().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "0Ap0O1twZSTC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "# Save model\n",
        "model_path = '/content/drive/MyDrive/bi_msaae_model_sigma10_rgb.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Training complete. Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "5qM21Qa4ZUW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1ebfynMZXPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}